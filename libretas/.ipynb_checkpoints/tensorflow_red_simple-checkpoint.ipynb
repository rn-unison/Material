{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imagenes/rn3.png\" width=\"200\">\n",
    "<img src=\"http://www.identidadbuho.uson.mx/assets/letragrama-rgb-150.jpg\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Una red neuronal multicapa simple usando TensorFlow\n",
    "\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 27 de septiembre de 2017.\n",
    "\n",
    "\n",
    "\n",
    "En esta libreta se muestra el ejemplo básico para una red multicapa sencilla\n",
    "aplicada al conjunto de datos [MNIST](http://yann.lecun.com/exdb/mnist/).\n",
    "\n",
    "Esta libreta es básicamente una traducción del ejemplo\n",
    "desarrollado por [Aymeric Damien](https://github.com/aymericdamien/TensorFlow-Examples/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar datos\n",
    "\n",
    "Primero cargamos los archivos que se utilizan para el aprendizaje. Para otro tipo de problemas, es necesario hacer un proceso conocido como *Data Wrangling*, que normalmente se realiza con la ayuda de *Pandas*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que un aprendizaje tenga sentido es necesario tener bien separado un conjunto de datos de aprendizaje y otro de prueba (en caso de grandes conjuntos de datos es la opción). Como vemos tanto las imágenes como las etiquetas están separados en archivos de datos y de aprendizaje.\n",
    "\n",
    "El objeto `mnist` es un objeto tensorflow que contiene 3 objetos tipo tensorflow: *test*, *train* y *validation*, los cuales a su vez contienen *ndarrays* de *numpy*. La estructura es la misma para cada conjunto de datos. Veamos su estructura:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tipo de images: {}\".format(type(mnist.train.images)))\n",
    "print(\"Tipo de epochs_completed: {}\".format(type(mnist.train.epochs_completed)))\n",
    "print(\"Tipo de labels: {}\".format(type(mnist.train.labels)))\n",
    "print(\"Tipo de nest_batch: {}\".format(type(mnist.train.next_batch)))\n",
    "print(\"Tipo de num_examples: {}\".format(type(mnist.train.num_examples)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como generar el conjunto de datos para ser utilizado dentro de TensorFlow es objeto de otra libreta. Por el momento concentremonos en como hacer una red neuronal rápido y sin dolor.\n",
    "\n",
    "Sin embargo, vamos a ver unos cuantos datos que nos pueden ser de útilidad para la construcción de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Forma del ndarray con las imágenes: {}\".format(mnist.train.images.shape))\n",
    "print(\"Forma del ndarray con las etiquetas: {}\".format(mnist.train.labels.shape))\n",
    "print(\"-\" * 79)\n",
    "print(\"Número de imagenes de entrenamiento: {}\".format(mnist.train.images.shape[0]))\n",
    "print(\"Tamaño de las imagenes: {}\".format(mnist.train.images.shape[1]))\n",
    "print(\"Clases diferentes: {}\".format(mnist.train.labels.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construcción de la red neuronal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para hacer una red neuronal lo más genérica posible y que pdamos reutilizar en otros proyectos, vamos a establecer los parámetros base independientemente de la inicialización de la red, independientemente de la forma en que construimos la red. \n",
    "\n",
    "Comencemos por establecer una función genérica que nos forme una red neuronal con dos capas ocultas. No agrego más comentarios porque, con la experiencia de las libretas anteriores, la construcción de la red neuronal se explica sola."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_neuronal_dos_capas_ocultas(x, pesos, sesgos):\n",
    "    \"\"\"\n",
    "    Genera una red neuronal de dos capas para usar en TensorFlow\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    pesos: un diccionario con tres etiquetas: 'h1', 'h2' y 'ho'\n",
    "           en donde cada una es una tf.Variable conteniendo una \n",
    "           matriz de dimensión [num_neuronas_capa_anterior, num_neuronas_capa]\n",
    "                  \n",
    "    sesgos: un diccionario con tres etiquetas: 'b1', 'b2' y 'bo'\n",
    "            en donde cada una es una tf.Variable conteniendo un\n",
    "            vector de dimensión [numero_de_neuronas_capa]\n",
    "                   \n",
    "    Devuelve\n",
    "    --------\n",
    "    Un ops de tensorflow que calcula la salida de una red neuronal\n",
    "    con dos capas ocultas, y activaciones RELU.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Primera capa oculta con activación ReLU\n",
    "    capa_1 = tf.matmul(x, pesos['h1'])\n",
    "    capa_1 = tf.add(capa_1, sesgos['b1'])\n",
    "    capa_1 = tf.nn.relu(capa_1)\n",
    "    \n",
    "    # Segunda capa oculta con activación ReLU\n",
    "    capa_2 = tf.matmul(capa_1, pesos['h2'])\n",
    "    capa_2 = tf.add(capa_2, sesgos['b2'])\n",
    "    capa_2 = tf.nn.relu(capa_2)\n",
    "    \n",
    "    # Capa de salida con activación lineal\n",
    "    # En Tensorflow la salida es siempre lineal, y luego se especifica\n",
    "    # la función de salida a la hora de calcularla como vamos a ver \n",
    "    # más adelante\n",
    "    capa_salida = tf.matmul(capa_2, pesos['ho']) + sesgos['bo']\n",
    "    return capa_salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora necesitamos poder generar los datos de entrada a la red neuronal de\n",
    "alguna manera posible. Afortunadamente sabemos exactamente que necesitaos, así\n",
    "que vamos a hacer una función que nos genere las variables de peso y sesgo.\n",
    "\n",
    "Por el momento, y muy a la brava, solo vamos a generarlas con números aletorios con una \n",
    "distribución $\\mathcal{N}(0, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializa_pesos(entradas, n1, n2, salidas):\n",
    "    \"\"\"\n",
    "    Genera un diccionario con pesos  \n",
    "    para ser utilizado en la función red_neuronal_dos_capas_ocultas\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    entradas: Número de neuronas en la capa de entrada\n",
    "    \n",
    "    n1: Número de neuronas en la primer capa oculta\n",
    "    \n",
    "    n2: Número de neuronas en la segunda capa oculta\n",
    "    \n",
    "    salidas: Número de neuronas de salida\n",
    "    \n",
    "    Devuelve\n",
    "    --------\n",
    "    Dos diccionarios, uno con los pesos por capa y otro con los sesgos por capa\n",
    "    \n",
    "    \"\"\"\n",
    "    pesos = {\n",
    "        'h1': tf.Variable(tf.random_normal([entradas, n1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n1, n2])),\n",
    "        'ho': tf.Variable(tf.random_normal([n2, salidas]))\n",
    "    }\n",
    "    \n",
    "    sesgos = {\n",
    "        'b1': tf.Variable(tf.random_normal([n1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n2])),\n",
    "        'bo': tf.Variable(tf.random_normal([salidas]))\n",
    "    }\n",
    "    \n",
    "    return pesos, sesgos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos establecer los parámetros de la topología de la red neuronal. \n",
    "Tomemos en cuenta que estos prámetros los podríamos haber establecido desde\n",
    "la primer celda, si el fin es estar variando los parámetros para escoger los que \n",
    "ofrezcan mejor desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_entradas = 784  #  Lo sabemos por la inspección que hicimos a mnist\n",
    "num_salidas = 10    # Ídem\n",
    "\n",
    "# Aqui es donde podemos jugar\n",
    "num_neuronas_capa_1 = 256\n",
    "num_neuronas_capa_2 = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡A construir la red! Para esto vamos a necesitar crear las entradas\n",
    "con un placeholder, y crear nuestra topología de red neuronal.\n",
    "\n",
    "Observa que la dimensión de x será [None, num_entradas], lo que significa que \n",
    "la cantidad de renglones es desconocida (o variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La entrada a la red neuronal\n",
    "x = tf.placeholder(\"float\", [None, num_entradas])\n",
    "\n",
    "# Los pesos y los sesgos\n",
    "w, b = inicializa_pesos(num_entradas, num_neuronas_capa_1, num_neuronas_capa_2, num_salidas)\n",
    "\n",
    "# Crea la red neuronal\n",
    "estimado = red_neuronal_dos_capas_ocultas(x, w, b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parecería que ya está todo listo. Sin ambargo falta algo muy importante: No hemos explicado\n",
    "ni cual es el criterio de error (loss) que vamos a utilizar, ni cual va a ser el método de\n",
    "optimización (aprendizaje) que hemos decidido aplicar.\n",
    "\n",
    "Primero definamos el costo que queremos minimizar, y ese costo va a estar en función de lo\n",
    "estimado con lo real, por lo que necesitamos otra entrada de datos para los datos de salida.\n",
    "\n",
    "Sin ningun lugar a dudas, el costo que mejor describe este problema es el de *softmax*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Creamos la variable de datos de salida conocidos\n",
    "y = tf.placeholder(\"float\", [None, num_salidas])\n",
    "\n",
    "#  Definimos la función de costo\n",
    "costo = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=estimado, labels=y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora definimos que función de aprendizaje vamos a utilizar. Existen muchas funciones\n",
    "de aprendizaje en tensorflow, las cuales se pueden consultar en `tf.train.`. Entre las\n",
    "existentes podemos ver algunas conocidas del curso como descenso de gradiente simple,\n",
    "momento, rprop, rmsprop entre otras. Casi todas las funciones de optimización (aprendizaje)\n",
    "acaban su nombre con `Optimize`.\n",
    "\n",
    "En este caso vamos a usar un método comocido como el *algoritmo de Adam* el cual \n",
    "se puede consultar [aqui](http://arxiv.org/pdf/1412.6980.pdf). El metodo utiliza dos calculos\n",
    "de momentos diferentes, y por lo visto genera resultados muy interesantes desde el punto \n",
    "de vista práctico.\n",
    "\n",
    "¿Cual es el mejor método? Pues esto es en función de tu problema y de la cantidad de datos que tengas.\n",
    "Lo mejor es practicar con varios métodos para entender sus ventajas y desventajas.\n",
    "\n",
    "En todo caso el método de optimización requiere que se le inicialice con una tasa de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alfa = 0.001\n",
    "optimizador = tf.train.AdamOptimizer(learning_rate=alfa)\n",
    "paso_entrenamiento = optimizador.minimize(costo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ejecutar la sesión usando mini-batches\n",
    "\n",
    "Ahora, ya que la red neuronal está lista vamos a ejecutar la red utilizando el algoritmo de\n",
    "Adam pero en forma de mini-batches. Con el fin de tener control sobre el problema, vamos a establecer un número máximo de epoch (ciclos de aprendizaje), el tamaño de los mini-batches, y cada cuandos epoch \n",
    "quisieramos ver como está evolucionando la red neuronal.\n",
    "\n",
    "Como entrenar una red neuronal no tiene sentido, si no es porque la queremos usar para reconocer,\n",
    "no tendría sentido entrenarla y luego perderla y tener que reentrenar en cada ocasión. Recuerda que cuando\n",
    "se cierra la sesión se borra todo lo que se tenía en memoria. \n",
    "\n",
    "Para esto vamos a usar una ops especial llamada `Saver`, que permite guardar en un archivo la red neuronal y \n",
    "después utilizarla en otra sesión (en otro script, computadora, ....).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_modelo = \"rnn2.ckpt\"\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como todo se ejecuta dentro de una sesión, no es posible hacerlo por partes (si usamos el \n",
    "`with` que debería ser la única forma en la que iniciaramos una sesión). Por lo tanto procuraré dejar comentado el código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numero_epochs = 30\n",
    "tamano_minibatch = 100\n",
    "display_step = 1\n",
    "\n",
    "# Muy importante la primera vez que se ejecuta inicializar todas las variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# La manera correcta de iniciar una sesión y realizar calculos\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Ciclos de entrenamiento\n",
    "    for epoch in range(numero_epochs):\n",
    "\n",
    "        #  Inicializa el costo promedio de todos los minibatches en 0\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        #  Calcula el número de minibatches que se pueden usar \n",
    "        total_batch = int(mnist.train.num_examples/tamano_minibatch)\n",
    "\n",
    "        #  Por cada minibatch\n",
    "        for i in range(total_batch):\n",
    "            \n",
    "            #  Utiliza un generador incluido en mnist que obtiene \n",
    "            #  tamano_minibatch ejemplos selecionados aleatoriamente del total\n",
    "            batch_x, batch_y = mnist.train.next_batch(tamano_minibatch)\n",
    "            \n",
    "            #  Ejecuta la ops del paso_entrenamiento para aprender \n",
    "            #  y la del costo, con el fin de mostrar el aprendizaje\n",
    "            _, c = sess.run([paso_entrenamiento, costo], feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "            #  Calcula el costo del minibatch y lo agrega al costo total\n",
    "            avg_cost += c / total_batch\n",
    "        \n",
    "        # Muestra los resultados\n",
    "        if epoch % display_step == 0:\n",
    "            print ((\"Epoch: \" + str(epoch)).ljust(20)\n",
    "                   + (\"Costo: \" + str(avg_cost)))\n",
    "    \n",
    "    #  Guarda la sesión en el archivo rnn2.cptk\n",
    "    saver.save(sess, archivo_modelo)\n",
    "    \n",
    "    print(\"Se acabaron los epochs, saliendo de la sesión de tensorflow.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ahora vamos a revisar que tan bien realizó el aprendizaje cuando se aplica la red adatos que\n",
    "no se usaron para entrenamiento. Para esto vamos a utilizar dos ops extas: una \n",
    "para definir la operaración de datos bien estimados o mal estimados, y otra para\n",
    "calcular el promedio de datos bien estimados. Para calcular los datos bien estimados vamos a utilizar `tf.cast` que permite ajustar los tipos\n",
    "al tipo tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_correcta = tf.equal(tf.argmax(estimado, 1), tf.argmax(y, 1))\n",
    "\n",
    "precision = tf.reduce_mean(tf.cast(prediction_correcta, \"float\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora si, vamos a abrir una nueva sesión, vamos a restaurar los valores de la sesión anterior,\n",
    "y vamos a ejecutar el grafo con el fin de evaluar la ops precision, pero ahora con el\n",
    "diccionario de alimentación con los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, archivo_modelo)\n",
    "    porcentaje_acierto = sess.run(precision, feed_dict={x: mnist.test.images,\n",
    "                                                        y: mnist.test.labels})\n",
    "    print(\"Precisión: {}\".format(porcentaje_acierto))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Contesta las siguientes preguntas\n",
    "\n",
    "1. ¿Que pasa si aumenta el número de epochs? ¿Cuando deja de ser util aumentar los epoch?\n",
    "\n",
    "2. ¿Que pasa si aumentas o disminuyes la tasa de aprendizaje?\n",
    "\n",
    "3. Utiliza al menos otros 2 métodos de optimización (existentes en Tensorflow), ajustalos y comparalos. ¿Cual de los métodos te gusta más y porque preferirías unos sobre otros?\n",
    "\n",
    "4. ¿Que pasa si cambias el tamaño de los minibatches?\n",
    "5. ¿Como harías si dejaste a medias un proceso de aprendizaje (en 10 epochs por ejemplo) y quisieras entrenar la red 10 epoch más, y mañana quisieras entrenarla otros 10 epoch más?\n",
    "\n",
    "**Para contestar las preguntas, agrega cuantas celdas con comentarios y con códgo sean necesarias.** Aprovecha que las libretas de *Jupyter* te permite hacerte una especie de tutorial personalizado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
