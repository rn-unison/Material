{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Universidad de Sonora](http://www.identidadbuho.uson.mx/assets/letragrama-rgb-72.jpg)\n",
    "## Ciencias de la Computación\n",
    "### [Curso de Redes Neuronales](https://curso-redes-neuronales-unison.github.io/Temario/)\n",
    "\n",
    "# Una sola neurona logística\n",
    "\n",
    "[**Julio Waissman Vilanova**](http://mat.uson.mx/~juliowaissman/), 6 de septiembre de 2017.\n",
    "\n",
    "En esta libreta vamos a revisar los aspectos básicos del aprendizaje para una sola neurona de salida logística, sin capas ocultas y usando el criterio de pérdida de entropia en dos clases. El algoritmo es sencillo pero es importante entenderlo bien antes de pasar a cosas más complicadas.\n",
    "\n",
    "Empecemos por inicializar los modulos que vamos a requerir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image  # Esto es para desplegar imágenes en la libreta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Función logística, función de pérdida y gradiente de la función de costo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función logística está dada por \n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1 + e^{-z}},\n",
    "$$\n",
    "\n",
    "la cual es importante que podamos calcular en forma vectorial. Si bien el calculo es de una sola linea, el uso de estas funciones auxiliares facilitan la legibilidad del código.\n",
    "\n",
    "#### Ejercicio 1: Desarrolla la función logística, la cual se calcule para todos los elementos de un ndarray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def logistica(z):\n",
    "    \"\"\"\n",
    "    Calcula la función logística para cada elemento de z\n",
    "    \n",
    "    @param z: un ndarray\n",
    "    @return: un ndarray de las mismas dimensiones que z\n",
    "    \"\"\"\n",
    "    # Introduce código aqui (una linea de código)\n",
    "    #---------------------------------------------------\n",
    "    return \n",
    "    #---------------------------------------------------\n",
    "    \n",
    "# prueba que efectivamente funciona la función implementada\n",
    "# si el assert es falso regresa un error de aserción (el testunit de los pobres)\n",
    "assert (np.abs(logistica(np.array([-1, 0, 1])) - np.array([ 0.26894142, 0.5, 0.73105858]))).sum() < 1e-6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar la función vamos a graficar la función logística en el intervalo [-5, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "z = np.linspace(-5, 5, 100)\n",
    "with plt.xkcd():\n",
    "    plt.plot( z, logistica(z))\n",
    "    plt.title(u'Función logística', fontsize=20)\n",
    "    plt.xlabel(r'$z$', fontsize=20)\n",
    "    plt.ylabel(r'$\\frac{1}{1 + \\exp(-z)}$', fontsize=26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez establecida la función logística, vamos a implementar la función de pérdida *sin regularizar* que se utiliza típicamente en clasificación binaria, la cual está dada por\n",
    "\n",
    "$$\n",
    "loss(y, \\hat{y}) = -\\frac{1}{T} \\sum_{i=1}^T \\left[ y^{(i)}\\log(\\hat{y}^{(i)}) + (1 - y^{(i)})\\log(1 - \\hat{y}^{(i)})\\right],\n",
    "$$\n",
    "\n",
    "donde \n",
    "\n",
    "$$\n",
    "\\hat{y}^{(i)} = g(\\omega^T x_e^{(i)}),\n",
    "$$\n",
    "\n",
    "las cuales fueron ecuaciones revisadas en clase.\n",
    "\n",
    "#### Ejercicio 2: Implementa la función de pérdida para un conjunto de aprendizaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perdida(w, x, y):\n",
    "    \"\"\"\n",
    "    Calcula el costo de una w dada para el conjunto dee entrenamiento dado por y y x\n",
    "    \n",
    "    @param w: un ndarray de dimensión (n + 1, 1) \n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    @param y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    \n",
    "    @return: un flotante con el costo\n",
    "    \n",
    "    \"\"\" \n",
    "    T = x.shape[0]\n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    # Agregua aqui tu código\n",
    "\n",
    "    \n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "# Otra vez el testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n",
    "w = np.ones((2,1))\n",
    "\n",
    "x = np.array([[1, 10],\n",
    "              [1, -5]])\n",
    "\n",
    "y1 = np.array([[1],\n",
    "               [0]])\n",
    "\n",
    "y2 = np.array([[0],\n",
    "               [1]])\n",
    "\n",
    "y3 = np.array([[0],\n",
    "               [0]])\n",
    "\n",
    "y4 = np.array([[1],\n",
    "               [1]])\n",
    "\n",
    "assert abs(perdida(w, x, y1) - 0.01) < 1e-2\n",
    "assert abs(perdida(w, x, y2) - 7.5) < 1e-2\n",
    "assert abs(perdida(w, x, y3) - 5.5) < 1e-2\n",
    "assert abs(perdida(w, x, y4) - 2.0) < 1e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la misma manera, para poder implementar las funciones de aprendizaje, vamos a implementar el gradiente de la función de pérdida. El gradiente de la función de pérdida respecto a $\\omega$ es (como lo vimos en clase) el siguiente:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial Loss(\\omega)}{\\partial \\omega_j} = -\\frac{1}{T} \\sum_{i=1}^T \\left(y^{(i)} - h_\\omega(x^{(i)})\\right)x_j^{(i)} \n",
    "$$\n",
    "\n",
    "y a partir de las ecuaciones individuales de puede obtener $\\nabla Loss(\\omega)$, la cual no la vamos a escribir en la libreta para que revisen en sus notas como se puede resolver este problema en forma matricial. Si bien para el descenso de gradiente podemos utilizar directamente el gradiente negado, al implementar métodos de optimización avanzados, necesitamos que el gradiente sea efectivamente el gradiente.\n",
    "\n",
    "#### Ejercicio 3: Implementa (con operaciones matriciales) el calculo del gradiente de la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradiente(w, x, y):\n",
    "    \"\"\"\n",
    "    Calcula el gradiente de la función de pérdida para clasificación binaria, \n",
    "    utilizando una neurona logística, para una theta y conociendo un conjunto de aprendizaje.\n",
    "    \n",
    "    @param w: un ndarray de dimensión (n + 1, 1) \n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    @param y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    \n",
    "    @return: un ndarray de mismas dimensiones que w\n",
    "    \n",
    "    \"\"\"\n",
    "    T = x.shape[0]\n",
    "\n",
    "    #------------------------------------------------------------------------\n",
    "    # Agregua aqui tu código\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    #------------------------------------------------------------------------\n",
    "    \n",
    "# Otra vez el testunit del pobre (ya lo calcule yo, pero puedes hacerlo a mano para estar seguro)\n",
    "w = np.ones((2, 1))\n",
    "\n",
    "x = np.array([[1, 10],\n",
    "              [1, -5]])\n",
    "\n",
    "y1 = np.array([[1],\n",
    "               [0]])\n",
    "\n",
    "y2 = np.array([[0],\n",
    "               [1]])\n",
    "\n",
    "y3 = np.array([[0],\n",
    "               [0]])\n",
    "\n",
    "y4 = np.array([[1],\n",
    "               [1]])\n",
    "\n",
    "assert abs(0.00898475 - gradiente(w, x, y1)[0]) < 1e-4\n",
    "assert abs(7.45495097 - gradiente(w, x, y2)[1]) < 1e-4 \n",
    "assert abs(4.95495097 - gradiente(w, x, y3)[1]) < 1e-4 \n",
    "assert abs(-0.49101525 - gradiente(w, x, y4)[0]) < 1e-4     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Descenso de gradiente y el método BGFS para regresión logística"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a desarrollar las funciones necesarias para realizar el entrenamiento y encontrar la mejor $\\omega$ de acuero a la función de costos y un conjunto de datos de aprendizaje.\n",
    "\n",
    "Para este problema, vamos a utilizar una base de datos sintética proveniente del curso de [Andrew Ng](www.andrewng.org/) que se encuentra en [Coursera](https://www.coursera.org). Supongamos que pertenecemos al departamente de servicios escolares de la UNISON y vamos a modificar el procedimiento de admisión. En lugar de utilizar un solo exámen (EXCOBA) y la información del cardex de la preparatoria, hemos decidido aplicar dos exámenes, uno sicométrico y otro de habilidades estudiantiles. Dichos exámenes se han aplicado el último año aunque no fueron utilizados como criterio. Así, tenemos un historial entre estudiantes aceptados y resultados de los dos exámenes. El objetivo es hacer un método de regresión que nos permita hacer la admisión a la UNISON tomando en cuenta únicamente los dos exámenes y simplificar el proceso. *Recuerda que esto no es verdad, es solo un ejercicio*.\n",
    "\n",
    "Bien, los datos se encuentran en el archivo `admision.txt` el cual se encuentra en formato `cvs` (osea los valores de las columnas separados por comas. Vamos a leer los datos y graficar la información para entender un poco los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datos = np.loadtxt('datos/admision.txt', comments='%', delimiter=',')\n",
    "\n",
    "x, y = datos[:,0:-1], datos[:,-1:] \n",
    "x = np.c_[np.ones((x.shape[0], 1)), x]\n",
    "\n",
    "plt.plot(x[y.ravel() == 1, 1], x[y.ravel() == 1, 2], 'sr', label='aceptados') \n",
    "plt.plot(x[y.ravel() == 0, 1], x[y.ravel() == 0, 2], 'ob', label='rechazados')\n",
    "plt.title(u'Ejemplo sintético para regresión logística')\n",
    "plt.xlabel(u'Calificación del primer examen')\n",
    "plt.ylabel(u'Calificación del segundo examen')\n",
    "plt.axis([20, 100, 20, 100])\n",
    "plt.legend(loc=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vistos los datos un clasificador lineal podría ser una buena solución. Ahora vamos a implementar el método de descenso de gradiente, casi de la misma manera que lo implementamos para regresión lineal (por lotes)\n",
    "\n",
    "#### Ejercicio 4: Implementa el descenso de gradiente para el problema de regresión logística en modo batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def descenso_rl_lotes(x, y, epsilon, tol=1e-4, max_iter=int(1e4), historial=False):\n",
    "    \"\"\"\n",
    "    Descenso de gradiente por lotes para resolver el problema de regresión logística con un conjunto de aprendizaje\n",
    "\n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "    @param y: un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    @param epsilon: Un flotante (típicamente pequeño) con la tasa de aprendizaje\n",
    "    @param tol: Un flotante pequeño como criterio de paro. Por default 1e-4\n",
    "    @param max_iter: Máximo numero de iteraciones. Por default 1e4\n",
    "    @param historial: Un booleano para saber si guardamos el historial de la función de pérdida o no\n",
    "    \n",
    "    @return: w, perdida_hist donde w es ndarray de dimensión (n + 1, 1) y perdida_hist es un\n",
    "             ndarray de dimensión (max_iter,) con el valor de la función de pérdida en cada iteración. \n",
    "             Si historial == True, entonces perdida_hist = None.\n",
    "             \n",
    "    \"\"\"\n",
    "    T, n = x.shape[0], x.shape[1] - 1\n",
    "    \n",
    "    w = np.zeros((n + 1, 1))\n",
    "    perdida_hist = np.zeros(max_iter) if costos else None\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        #--------------------------------------------------------------\n",
    "        # Agregar aqui tu código\n",
    "        #\n",
    "        # Recuerda utilizar las funciones que ya has desarrollado\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #--------------------------------------------------------------\n",
    "\n",
    "    return w, costo_hist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para probar la función de aprendizaje, vamos a aplicarla a nuestro problema de admisión. Primero recuerda que tienes que hacer una exploración para encontrar el mejor valor de $\\epsilon$. Así que utiliza el código de abajo para ajustar $\\epsilon$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-4\n",
    "mi = 50\n",
    "_, perdida_hist = descenso_rl_lotes(x, y, epsilon, tol=1e-4, max_iter=mi, historial=True)\n",
    "\n",
    "plt.plot(np.arange(mi), perdida_hist)\n",
    "plt.title(r'Evolucion del valor de la funci\\'on de p\\'erdida en las primeras iteraciones con $\\epsilon$ = ' + str(epsilon))\n",
    "plt.xlabel('iteraciones')\n",
    "plt.ylabel('perdida')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez encontrado el mejor $\\epsilon$, entonces podemos calcular $\\omega$ (esto va a tardar bastante), recuerda que el costo final debe de ser lo más cercano a 0 posible, así que agrega cuantas iteraciones sean necesarias: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_lotes, _ = descenso_rl_lotes(x, y, epsilon, max_iter = int(1e6))\n",
    "print(\"Los pesos obtenidos son: \\n{}\".format(w_lotes))\n",
    "print(\"El valor final de la función de pérdida es: {}\".format(perdida(w_lotes, x, y))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es interesante ver como el descenso de gradiente no es eficiente en este tipo de problemas, a pesar de ser problemas de optimización convexos.\n",
    "\n",
    "Bueno, este método nos devuelve $\\omega$, pero esto no es suficiente para decir que tenemos un clasificador, ya que un método de clasificación se compone de dos métodos, uno para **aprender** y otro para **predecir**. Recuerda que para realizar la predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5: Desarrolla una función de predicción."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictor(w, x):\n",
    "    \"\"\"\n",
    "    Predice los valores de y_hat (que solo pueden ser 0 o 1), utilizando el criterio MAP.\n",
    "    \n",
    "    @param w: un ndarray de dimensión (n + 1, 1)\n",
    "    @param x: un ndarray de dimensión (T, n + 1) donde la primer columna son puros unos\n",
    "\n",
    "    @return: y_hat un ndarray de dimensión (T, 1) donde cada entrada es 1.0 o 0.0\n",
    "    \"\"\"\n",
    "    #-------------------------------------------------------------------------------------\n",
    "    # Agrega aqui tu código sin utilizar la función logística\n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------------------------------------\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Que tan bueno es este clasificador? ¿Es que implementamos bien el método?\n",
    "\n",
    "Vamos a contestar esto por partes. Primero, vamos a graficar los mismos datos pero vamos a agregar la superficie de separación, la cual en este caso sabemos que es una linea recta. Como sabemos el criterio para decidir si un punto pertenece a la clase 1 o cero es si el valor de $\\omega^T x_e^{(i)} \\ge 0$, por lo que la frontera entre la región donde se escoge una clase de otra se encuentra en:\n",
    "\n",
    "$$\n",
    "0 = \\omega_0 + \\omega_1 x_1  + \\omega_2 x_2,\n",
    "$$\n",
    "\n",
    "y despejando:\n",
    "\n",
    "$$\n",
    "x_2 = -\\frac{\\omega_0}{\\omega_2} -\\frac{\\omega_1}{\\omega_2}x_1\n",
    "$$\n",
    "\n",
    "son los pares $(x_1, x_2)$ de valores en la forntera. Al ser estos (en este caso) una linea recta solo necesitamos dos para graficar la superficie de separación. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1_frontera = np.array([20, 100]) #Los valores mínimo y máximo que tenemos en la gráfica de puntos\n",
    "x2_frontera = -(w_lotes[0] / w_lotes[2]) - (w_lotes[1] / w_lotes[2]) * x1_frontera\n",
    "\n",
    "plt.plot(x[y.ravel() == 1, 1], x[y.ravel() == 1, 2], 'sr', label='aceptados') \n",
    "plt.plot(x[y.ravel() == 0, 1], x[y.ravel() == 0, 2], 'ob', label='rechazados')\n",
    "plt.plot(x1_frontera, x2_frontera, 'm')\n",
    "plt.title(u'Ejemplo sintético para regresión logística')\n",
    "plt.xlabel(u'Calificación del primer examen')\n",
    "plt.ylabel(u'Calificación del segundo examen')\n",
    "plt.axis([20, 100, 20, 100])\n",
    "plt.legend(loc=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y para que tengas una idea de lo que debería de salir, anexo una figura obtenida con el código que yo hice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(filename='ejemplo_logistica_1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bueno, ya vimos que el método del descenso de gradiente, si bien es correcto y fácil de implementar, es bastante ineficiente en cuanto a tiempo (inclusive para un problema bastante simple, por lo que vamos a utilizar la función `minimize` que provee `scipy` la que vamos a utilizar con el algoritmo BFGS. Ejecuta la celda de abajo para revisar la documentación de `minimize` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, el método BFGS es el método por default. Los parámetros que hay que agregar son:\n",
    "\n",
    "* Una $\\omega$ inicial en `x0`\n",
    "* Una funcion de costo $J(\\omega)$ en `fun`\n",
    "* Una función de gradiente $\\nabla J(\\omega)$ en `jac`\n",
    "* Argumentos extras para `fun` y `jac` en forma de tupla\n",
    "* Opciones del método de optimización en `options` como un diccionario. Las opciones pueden ser:\n",
    "  * gtol: Valor de tolerancia en la variación de la norma del gradiente\n",
    "  * maxiter: Máximo numero de iteraciones\n",
    "  * disp: Si True, despliega la información sobre la convergencia del algoritmo\n",
    "\n",
    "La función regresa un objeto resultado `res`, del cual solo nos interesa `res.x`, el resultado de theta.\n",
    "\n",
    "Como $\\omega$ en este método debe de ser un ndim de una sola dimensión, y nosotros hemos estado usando $\\theta$ como un vector columna pues podemos modificar un poco estas funciones con unas nuevas.\n",
    "\n",
    "Veras que si lo ejecutas varias veces, en muchas ocasiones vamos a tener que el resultado no se pudo lograr por falta de precisión en la función del gradiente. Esto es debido a la forma en que estamos calculando el gradiente, y a que si la solución inicial realiza una partición pésima del estado, entonces hay problemas para estimar el inverso del hessiano, por problemas de estabilidad numérica del método. Por esta razón se inicializa el algoritmo con un valor de $\\omega$ obtenido en forma aleatoria (valores pequeños).\n",
    "\n",
    "Lo interesante es que el algoritmo avisa cuando no hay convergencia, y cuando hay se obtiene en muy pocas iteraciones (19 en promedio)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w0 = 1e-2 * np.random.rand(x.shape[1])\n",
    "print theta0\n",
    "funcion = lambda w, x, y: perdida(w.reshape(-1,1), x, y)\n",
    "jacobiano = lambda w, x, y: gradiente(w.reshape(-1,1), x, y).ravel()\n",
    "res = minimize(x0=w0,\n",
    "               fun=funcion,\n",
    "               jac=jacobiano,\n",
    "               args = (x, y),\n",
    "               method='BFGS',\n",
    "               options= {'maxiter': 400, 'disp': True})\n",
    "print res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra forma de obtener el resultado es utilizando un método tipo *simplex* que no necesita calcular ni el jacobiano, ni la inversión del hessiano. Que si bien requiere sensiblemente de más iteraciones, para un problema relativamente simple como éste, es suficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w0 = np.zeros(x.shape[1])\n",
    "print theta0\n",
    "funcion = lambda w, x, y: costo(w.reshape(-1,1), x, y)\n",
    "jacobiano = lambda w, x, y: gradiente(w.reshape(-1,1), x, y).ravel()\n",
    "res = minimize(x0=theta0,\n",
    "               fun=funcion,\n",
    "               args = (x, y),\n",
    "               method='Nelder-Mead',\n",
    "               options= {'maxiter': 400, 'disp': True})\n",
    "print res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora veamos la partición del espacio con este método, la cual es ligeramente diferente a la obtenida con el descenso de gradiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_NM = res.x.reshape(-1,1)\n",
    "\n",
    "x1_frontera = np.array([20, 100]) #Los valores mínimo y máximo que tenemos en la gráfica de puntos\n",
    "x2_frontera = -(w_NM[0] / w_NM[2]) - (w_NM[1] / w_NM[2]) * x1_frontera\n",
    "\n",
    "plt.plot(x[y.ravel() == 1, 1], x[y.ravel() == 1, 2], 'sr', label='aceptados') \n",
    "plt.plot(x[y.ravel() == 0, 1], x[y.ravel() == 0, 2], 'ob', label='rechazados')\n",
    "plt.plot(x1_frontera, x2_frontera, 'm')\n",
    "plt.title(u'Ejemplo sintético para regresión logística')\n",
    "plt.xlabel(u'Calificación del primer examen')\n",
    "plt.ylabel(u'Calificación del segundo examen')\n",
    "plt.axis([20, 100, 20, 100])\n",
    "plt.legend(loc=0)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
